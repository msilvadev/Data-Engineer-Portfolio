<h1 align="center">Data Modeling with PostgreSQL</h1>

[![author](https://img.shields.io/badge/author-Matheus-red.svg)](https://www.linkedin.com/in/msilvadev/) ![](https://img.shields.io/badge/technology-Python-blue.svg) ![](https://img.shields.io/badge/database-PostgreSQL-blue.svg)

<h2 align="center">Summary</h2>

A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity 
on their new music streaming app. The analytics team is particularly interested in understanding what 
songs users are listening to. Currently, they don't have an easy way to query their data, which resides 
in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the 
songs in their app.

In this project, you'll apply what you've learned on data modeling with Postgres and build an ETL pipeline using Python. 
To complete the project, you will need to define fact and dimension tables for a star schema for a particular analytic 
focus, and write an ETL pipeline that transfers data from files in two local directories into these tables in Postgres 
using Python and SQL.

<h2 align="center">Role performed</h2>
I played the role of Data Engineer where I could create a Postgres database, since it data modeling until the ETL process. 
This database was designed to optimize queries on song play analysis.

The steps which I followed was:

    Create database schema. (DDL's are in the file called sql_queries.py)
    Create ETL pipeline

<h2 align="center">Schema for Song Play Analysis</h2>
<p align="center">
  <img src="sparkify_star_schema.png" width="644" height="757">
</p>

<h2 align="center">Detailing files from project</h2>

### Datasets
* The first called **song_data** which are in `data/song_data/*`. This dataset is a subset of real data from the Million Song Dataset. 
  Each file is in JSON format and contains metadata about a song and the artist of that song.


* The second dataset called **log_data** which are in `data/log_data/*`. Consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. 
  These simulate activity logs from a music streaming app based on specified configurations.

### [create_tables](create_tables.py)
Drops and creates your tables. You run this file to reset your tables before each time you run your ETL scripts.

### [etl.ipynb](etl.ipynb)
Jupyter Notebook is the file that reads and processes a single file from **song_data** and **log_data** and loads the data into
your tables also were util to explore, understand all processes that I need to perform.

### [etl.py](etl.py)
Code that reads and processes all files from **song_data** and **log_data** and loads the data into tables.

### [sql_queries.py](sql_queries.py)
This file contains DDL to `DROP` and `CREATE` all necessary table. Also has DML with all necessary `INSERT'S` and has `SELECT`.

### [test.ipynb](test.ipynb)
Connects to the database and picked up the first rows of each table to allow you to check the data loaded.

### [data_quality_analysis.ipynb](data_quality_analysis.ipynb)
Jupyter Notebook that performs makes data quality analysis in the data datasets used.



<h2 align="center">Running</h2>
Required to have **Python 3** on the running machine. If you need to install it, you can check [here](https://www.python.org/downloads/)

Check out if database PostgreSql is running.

Now follow steps:
  
1. Create Tables
    *  Run [create_tables](create_tables.py) which will make `DROP` and `CREATE` the necessary tables. PS: Open file and check host dbname user password
        ```
        python create_tables.py
        ```

2. ETL Pipeline
    * Required that step 1 has been performed
    * Run [etl.py](etl.py)
      ```
      python etl.py
      ```
      * This script will connect with local PostgreSql on a schema *sparkifydb*
      * Will read and load all files from `data/song_data`
      * Will read and load all files from `data/log_data`
  
3. Query data
    * We can use [test.ipynb](test.ipynb) to execute `SELECT` in each table or execute the following querys in your favorite tool:
  
      ````
      SELECT * FROM songplays LIMIT 5;
      ````
      
      ````
      SELECT * FROM users LIMIT 5;
      ````
      
      ````
      SELECT * FROM songs LIMIT 5;
      ````

      ````
      SELECT * FROM artists LIMIT 5;
      ````

      ````
      SELECT * FROM time LIMIT 5;
      ````